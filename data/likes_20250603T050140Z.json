[
  {
    "_id": "63990f21cc50af73d29ecfa3",
    "id": "fka/awesome-chatgpt-prompts",
    "author": "fka",
    "disabled": false,
    "gated": false,
    "lastModified": "2025-01-06T00:02:53+00:00",
    "likes": 7825,
    "private": false,
    "sha": "68ba7694e23014788dcc8ab5afe613824f45a05c",
    "description": "🧠 Awesome ChatGPT Prompts [CSV dataset]\n\nThis is a Dataset Repository of Awesome ChatGPT Prompts\nView All Prompts on GitHub\n\n\t\n\t\t\n\t\tLicense\n\t\n\nCC-0\n",
    "downloads": 17686,
    "tags": [
      "task_categories:question-answering",
      "license:cc0-1.0",
      "size_categories:n<1K",
      "format:csv",
      "modality:text",
      "library:datasets",
      "library:pandas",
      "library:mlcroissant",
      "library:polars",
      "region:us",
      "ChatGPT"
    ],
    "createdAt": "2022-12-13T23:47:45+00:00",
    "key": "",
    "viewer_sample_rows": [
      {
        "row_idx": 0,
        "row": {
          "act": "An Ethereum Developer",
          "prompt": "Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 1,
        "row": {
          "act": "SEO Prompt",
          "prompt": "Using WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they’re not competing articles. Split the outline into part 1 and part 2."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 2,
        "row": {
          "act": "Linux Terminal",
          "prompt": "I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd"
        },
        "truncated_cells": []
      },
      {
        "row_idx": 3,
        "row": {
          "act": "English Translator and Improver",
          "prompt": "I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is \"istanbulu cok seviyom burada olmak cok guzel\""
        },
        "truncated_cells": []
      },
      {
        "row_idx": 4,
        "row": {
          "act": "`position` Interviewer",
          "prompt": "I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conversation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is \"Hi\""
        },
        "truncated_cells": []
      },
      {
        "row_idx": 5,
        "row": {
          "act": "JavaScript Console",
          "prompt": "I want you to act as a javascript console. I will type commands and you will reply with what the javascript console should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is console.log(\"Hello World\");"
        },
        "truncated_cells": []
      },
      {
        "row_idx": 6,
        "row": {
          "act": "Excel Sheet",
          "prompt": "I want you to act as a text based excel. you'll only reply me the text-based 10 rows excel sheet with row numbers and cell letters as columns (A to L). First column header should be empty to reference row number. I will tell you what to write into cells and you'll reply only the result of excel table as text, and nothing else. Do not write explanations. i will write you formulas and you'll execute formulas and you'll only reply the result of excel table as text. First, reply me the empty sheet."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 7,
        "row": {
          "act": "English Pronunciation Helper",
          "prompt": "I want you to act as an English pronunciation assistant for Turkish speaking people. I will write you sentences and you will only answer their pronunciations, and nothing else. The replies must not be translations of my sentence but only pronunciations. Pronunciations should use Turkish Latin letters for phonetics. Do not write explanations on replies. My first sentence is \"how the weather is in Istanbul?\""
        },
        "truncated_cells": []
      },
      {
        "row_idx": 8,
        "row": {
          "act": "Spoken English Teacher and Improver",
          "prompt": "I want you to act as a spoken English teacher and improver. I will speak to you in English and you will reply to me in English to practice my spoken English. I want you to keep your reply neat, limiting the reply to 100 words. I want you to strictly correct my grammar mistakes, typos, and factual errors. I want you to ask me a question in your reply. Now let's start practicing, you could ask me a question first. Remember, I want you to strictly correct my grammar mistakes, typos, and factual errors."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 9,
        "row": {
          "act": "Travel Guide",
          "prompt": "I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion request is \"I am in Istanbul/Beyoğlu and I want to visit only museums.\""
        },
        "truncated_cells": []
      }
    ],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "66212f29fb07c3e05ad0432e",
    "id": "HuggingFaceFW/fineweb",
    "author": "HuggingFaceFW",
    "disabled": false,
    "gated": false,
    "lastModified": "2025-01-31T14:10:44+00:00",
    "likes": 2177,
    "private": false,
    "sha": "0f039043b23fe1d4eed300b504aa4b4a68f1c7ba",
    "description": "\n\t\n\t\t\n\t\t🍷 FineWeb\n\t\n\n\n    \n\n\n\n15 trillion tokens of the finest data the 🌐 web has to offer\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nThe 🍷 FineWeb dataset consists of more than 15T tokens of cleaned and deduplicated english web data from CommonCrawl. The data processing pipeline is optimized for LLM performance and ran on the 🏭 datatrove library, our large scale data processing library. \n🍷 FineWeb was originally meant to be a fully open replication of 🦅 RefinedWeb, with a release of the full dataset under… See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb.",
    "downloads": 399281,
    "tags": [
      "task_categories:text-generation",
      "language:en",
      "license:odc-by",
      "size_categories:10B<n<100B",
      "format:parquet",
      "modality:tabular",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2306.01116",
      "arxiv:2109.07445",
      "arxiv:2406.17557",
      "doi:10.57967/hf/2493",
      "region:us"
    ],
    "createdAt": "2024-04-18T14:33:13+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Tabular",
      "Text"
    ]
  },
  {
    "_id": "648b556b363cf923caddc497",
    "id": "Open-Orca/OpenOrca",
    "author": "Open-Orca",
    "disabled": false,
    "gated": false,
    "lastModified": "2025-02-19T07:32:36+00:00",
    "likes": 1408,
    "private": false,
    "sha": "e9c87b4abb2609913751f9b26553fdb9c061796c",
    "description": "🐋 The OpenOrca Dataset! 🐋\n\n\n\nWe are thrilled to announce the release of the OpenOrca dataset!\nThis rich collection of augmented FLAN data aligns, as best as possible, with the distributions outlined in the Orca paper.\nIt has been instrumental in generating high-performing model checkpoints and serves as a valuable resource for all NLP researchers and developers!\n\n\t\n\t\t\n\t\n\t\n\t\tOfficial Models\n\t\n\n\n\t\n\t\n\t\n\t\tMistral-7B-OpenOrca\n\t\n\nOur latest model, the first 7B to score better overall than all… See the full description on the dataset page: https://huggingface.co/datasets/Open-Orca/OpenOrca.",
    "downloads": 9262,
    "tags": [
      "task_categories:text-classification",
      "task_categories:token-classification",
      "task_categories:table-question-answering",
      "task_categories:question-answering",
      "task_categories:zero-shot-classification",
      "task_categories:summarization",
      "task_categories:feature-extraction",
      "task_categories:text-generation",
      "task_categories:text2text-generation",
      "language:en",
      "license:mit",
      "size_categories:1M<n<10M",
      "format:parquet",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2306.02707",
      "arxiv:2301.13688",
      "arxiv:2302.13971",
      "region:us"
    ],
    "createdAt": "2023-06-15T18:16:11+00:00",
    "key": "",
    "viewer_sample_rows": [
      {
        "row_idx": 0,
        "row": {
          "id": "niv.242684",
          "system_prompt": "",
          "question": "You will be given a definition of a task first, then some input of the task.\nThis task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets accurately capture the structure and semantics of the input sentence. The input is a sentence and the output is a list of triplets of the form [subject, predicate, object] that capture the relationships present in the sentence. When a sentence has more than 1 RDF triplet possible, the output must contain all of them.\n\nAFC Ajax (amateurs)'s ground is Sportpark De Toekomst where Ajax Youth Academy also play.\nOutput:",
          "response": "[\n  [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"],\n  [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]\n]"
        },
        "truncated_cells": []
      },
      {
        "row_idx": 1,
        "row": {
          "id": "flan.564327",
          "system_prompt": "You are an AI assistant. You will be given a task. You must generate a detailed and long answer.",
          "question": "Generate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One",
          "response": "Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 2,
        "row": {
          "id": "flan.1875913",
          "system_prompt": "You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
          "question": "What happens next in this paragraph?\n\nShe then rubs a needle on a cotton ball then pushing it onto a pencil and wrapping thread around it. She then holds up a box of a product and then pouring several liquids into a bowl. she\nChoose your answer from: A. adds saucepan and shakes up the product in a grinder. B. pinches the thread to style a cigarette, and then walks away. C. then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. D. begins to style her hair and cuts it several times before parting the ends of it to show the hairstyle she has created.",
          "response": "C. She then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. In this option, she is continuing the process of using the needle, pencil, and thread, which is most related to what she was doing in the previous sentence."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 3,
        "row": {
          "id": "t0.408370",
          "system_prompt": "You are an AI assistant. You will be given a task. You must generate a detailed and long answer.",
          "question": "Please answer the following question: I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\"?\nAnswer:",
          "response": "Based on the passage, discuss the primary motivations and outcomes of the 1901 Federation of Australia, including the roles and responsibilities of the federal government, as well as the continued governmental structures of the individual states involved."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 4,
        "row": {
          "id": "cot.86217",
          "system_prompt": "You are an AI assistant that helps people find information.",
          "question": "James runs a TV show and there are 5 main characters and 4 minor characters. He pays the minor characters $15,000 each episode. He paid the major characters three times as much. How much does he pay per episode? Let's be accurate as possible.",
          "response": "James pays the minor characters $15,000 each episode. Since there are 4 minor characters, he pays them a total of 4 * $15,000 = $60,000 per episode.\n\nThe major characters are paid three times as much. So, each major character gets paid 3 * $15,000 = $45,000 per episode.\n\nThere are 5 main characters, so he pays them a total of 5 * $45,000 = $225,000 per episode.\n\nIn total, James pays $225,000 (major characters) + $60,000 (minor characters) = $285,000 per episode."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 5,
        "row": {
          "id": "cot.18180",
          "system_prompt": "You are an AI assistant that helps people find information.",
          "question": "Given the stream of consciousness rationale, provide a reasonable question and answer. Rationale: Here the game refers to a volleyball where female beach volleyball player serving the ball.\n The question and answer:",
          "response": "Question: What is the proper technique for a female beach volleyball player to serve the ball effectively in a game?\n\nAnswer: To serve the ball effectively in beach volleyball, a female player should utilize a consistent pre-serve routine, stand in a balanced position with feet shoulder-width apart, toss the ball at an appropriate height while extending their non-hitting arm, and use a combination of shoulder, arm, and wrist movement to achieve a powerful and accurate serve. Different serve styles, such as underhand, topspin, and float, can be used depending on the player's strengths and the desired outcome."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 6,
        "row": {
          "id": "flan.2136716",
          "system_prompt": "You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
          "question": "Multi-choice question: What is the sentiment of the following tweet?\nTweet: @nikkigreen I told you \nChoose your answer from:\n + negative;\n + positive;",
          "response": "To determine the sentiment of the tweet, we need to analyze it thoroughly.\n\nTweet: @nikkigreen I told you\n\nStep 1: Identify the words or phrases that carry emotional weight.\nIn this tweet, there is only one phrase worth examining: \"I told you.\"\n\nStep 2: Determine the sentiment of the identified words or phrases.\n\"I told you\" can carry a variety of sentiments, depending on the context. It could be positive, negative, or neutral.\n\nStep 3: Consider the overall context of the tweet.\nUnfortunately, without more context, it is impossible to determine the exact sentiment of the tweet.\n\nAs a result, we cannot confidently choose an answer from the provided options, positive or negative, without more contextual information."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 7,
        "row": {
          "id": "cot.84626",
          "system_prompt": "You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-bystep and justify your answer.",
          "question": "John was a terrible writer.  To practice, his teacher suggest that he consider people he knows and do what?\nOptions:\n- write novels\n- advertising firm\n- write letter\n- write notes\n- write poems\nLet's think now! Step-by-step reasoning:",
          "response": "Step 1: Analyze the situation - John is a terrible writer and needs practice to improve his skills. His teacher gives him advice on how to practice.\n\nStep 2: Consider the options given:\n- Write novels: This might be a bit overwhelming for someone who is just starting to improve their writing skills, as novels are long and require a lot of commitment.\n- Advertising firm: This option doesn't seem to directly connect to practicing writing, as it refers to a type of business rather than a writing activity.\n- Write letters: Writing letters could be a good way to practice writing, especially since John can think about people he knows while doing so. This makes the writing more personal and engaging.\n- Write notes: Writing notes can help improve writing skills to some extent, but might not allow for the kind of continuous practice that John needs.\n- Write poems: This is another creative option to practice writing, but it might not be the best choice if John struggles with the basics of writing and is not yet comfortable with more advanced forms such as poetry.\n\nStep 3: Choose the most suitable option based on the analysis:\nGiven the options, writing letters seems to be the best advice for John to follow when considering people he knows. This would allow him to practice writing in a personal, engaging way while focusing on communicating with others, which could help him improve his skills."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 8,
        "row": {
          "id": "flan.537671",
          "system_prompt": "You are an AI assistant. You will be given a task. You must generate a detailed and long answer.",
          "question": "What is a one-sentence summary of the following article?\nOne of the first complaints presented to doctors by many people later diagnosed with Parkinson’s Disease is an involuntary tremor or shaking of the hands, fingers, arms, legs, jaw, and face. You’re most likely to notice this tremor when your hands are at rest rather than when you’re using them, although you may see more tremors while your hands and arms are in use as the disease progresses.  There are many causes of tremors. Parkinson's Disease is one of the most common causes, and tremors are often the first sign of the disease. The tremor and other symptoms may initially appear only on one side of the body, or they may appear worse on one side than the other. A repetitive “pill-rolling\" movement between the thumb and finger—named because it looks as though the person is rolling a pill between their fingers—is characteristic of a Parkinsonian tremor. Some symptoms of Parkinson's are caused by the larger symptom of slowed movements (also known as bradykinesia). This primarily affects motor functions from walking and balance to writing and even motor functions that are often considered reflexive or spontaneous.  These slowed movements are a very common early sign of Parkinson’s, and may show up at the onset of the disease in 80% of patients. Some people might have a hard time describing what they are feeling and use words like “weakness,” “tiredness,” or “incoordination” when talking about these symptoms. Look for distortions in voluntary movements. In addition to involuntary movements, those with Parkinson’s may experience disturbances in their voluntary movements beyond just slowness. Some of the treatments used for Parkinson's disease may cause abnormal involuntary movements, or an increase in movement, referred to as dyskinesia. These distortions (dyskinesias) can be similar to a “tic” in appearance and worsen with psychological stress. Advanced dyskinesia is most often seen in patients who have been on the medication Levodopa for some time. A common symptom of Parkinson’s is a shuffling gait with short steps and a tendency to lean forward. People with Parkinson's often have difficulty with their balance, and there is sometimes a tendency to fall forward, causing the person to walk faster and faster to keep from falling over. This pattern is called a \"festinating gait,\" and it is very common in Parkinson's disease. This symptom typically shows up in the later stages of the disease. Those with Parkinson’s Disease often lean forward at the waist when standing or walking. This is because Parkinson's can cause posture and balance problems, including rigidity. There is a tendency to flex the arms and the head so that the person appears bent over with the elbows flexed and the head down.  Rigidity can affect the whole body, and might you feel stiff or sore. Check for postural rigidity. Also referred to as “cog-wheeling,” this symptom presents as a hallmark, rigid motion when an examiner moves the arm of a patient through basic flexion and extension motions. The rigidity and resistance to movement is most obvious with passive movement of the wrists and elbow. Cog-wheeling may happen when the increased muscle tone related to rigidity is combined with a tremor. Some cognitive impairment is common, but usually not until late in the course of the disease. Around 90% of those with PD will show signs of speech disturbances at some point. These can present as quieter speech, breathy or hoarse qualities to the voice, and reduced precision with language. The voice often becomes soft or whispery because the vocal muscles lack mobility. Up to 60% of those with PD may show signs or anxiety or depression. PD affects some of the mood-stabilizing portions of the brain, which leads to this increased chance for depression, especially paired with the quality of life for patients in late stages of the disease. The muscles used to push food through the digestive system are also affected by PD. This can lead to a variety of gastrointestinal issues from incontinence to constipation. These same symptoms often present along with difficulty swallowing food as well. Many of the involuntary movements associated with Parkinson’s make it increasingly difficult for those with the disease to get a full night of sleep. Other symptoms—such as muscle rigidity making it difficult to roll over at night or bladder issues leading to getting up at night to urinate frequently—compound the sleep disruptions experienced by those with PD.\n\nSummary:",
          "response": "Involuntary tremors, slowed movements, and posture and balance issues are common early symptoms of Parkinson's Disease, often accompanied by disturbances in voluntary movements, speech, mood, and gastrointestinal issues as the disease progresses."
        },
        "truncated_cells": []
      },
      {
        "row_idx": 9,
        "row": {
          "id": "flan.1317837",
          "system_prompt": "You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
          "question": "Premise: A man is inline skating in front of a wooden bench.\n\nHypothesis: A man is having fun skating in front of a bench.\n\n.Choose the correct answer: Given the premise, can we conclude the hypothesis?\n\nSelect from: a). yes b). it is not possible to tell c). no",
          "response": "b). it is not possible to tell\n\nJustification: Although the man is inline skating in front of the wooden bench, we cannot conclude whether he is having fun or not, as his emotions are not explicitly mentioned."
        },
        "truncated_cells": []
      }
    ],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "64382440c212a363c3ac15c8",
    "id": "OpenAssistant/oasst1",
    "author": "OpenAssistant",
    "disabled": false,
    "gated": false,
    "lastModified": "2023-05-02T13:21:21+00:00",
    "likes": 1391,
    "private": false,
    "sha": "fdf72ae0827c1cda404aff25b6603abec9e3399b",
    "description": "\n\t\n\t\t\n\t\tOpenAssistant Conversations Dataset (OASST1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \nis a product of a worldwide crowd-sourcing effort… See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst1.",
    "downloads": 8420,
    "tags": [
      "language:en",
      "language:es",
      "language:ru",
      "language:de",
      "language:pl",
      "language:th",
      "language:vi",
      "language:sv",
      "language:bn",
      "language:da",
      "language:he",
      "language:it",
      "language:fa",
      "language:sk",
      "language:id",
      "language:nb",
      "language:el",
      "language:nl",
      "language:hu",
      "language:eu",
      "language:zh",
      "language:eo",
      "language:ja",
      "language:ca",
      "language:cs",
      "language:bg",
      "language:fi",
      "language:pt",
      "language:tr",
      "language:ro",
      "language:ar",
      "language:uk",
      "language:gl",
      "language:fr",
      "language:ko",
      "license:apache-2.0",
      "size_categories:10K<n<100K",
      "format:parquet",
      "modality:tabular",
      "modality:text",
      "library:datasets",
      "library:pandas",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2304.07327",
      "region:us",
      "human-feedback"
    ],
    "createdAt": "2023-04-13T15:48:16+00:00",
    "key": "",
    "viewer_sample_rows": [
      {
        "row_idx": 0,
        "row": {
          "message_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "parent_id": null,
          "user_id": "c3fe8c76-fc30-4fa7-b7f8-c492f5967d18",
          "created_date": "2023-02-05T14:23:50.983374+00:00",
          "text": "Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.",
          "role": "prompter",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": null,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.00044308538781479,
            "severe_toxicity": 3.252684837207198e-05,
            "obscene": 0.00023475120542570949,
            "identity_attack": 0.0001416115992469713,
            "insult": 0.00039489680784754455,
            "threat": 4.075629112776369e-05,
            "sexual_explicit": 2.712695459194947e-05
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": {
            "name": [
              "+1",
              "_skip_reply",
              "_skip_ranking"
            ],
            "count": [
              10,
              1,
              4
            ]
          },
          "labels": {
            "name": [
              "spam",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "creativity",
              "violence"
            ],
            "value": [
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.9166666666666666,
              0.16666666666666666,
              0.3333333333333333,
              0.6666666666666666,
              0.0
            ],
            "count": [
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3
            ]
          }
        },
        "truncated_cells": []
      },
      {
        "row_idx": 1,
        "row": {
          "message_id": "c8e83833-ecbc-44fe-b6db-735228c25a1c",
          "parent_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "user_id": "2c96e467-66f0-4be7-9693-bda51356a424",
          "created_date": "2023-02-06T13:50:44.657083+00:00",
          "text": "\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n\nReferences:\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.",
          "role": "assistant",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": 0,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.00026396565954200923,
            "severe_toxicity": 2.7454958399175666e-05,
            "obscene": 0.00013584605767391622,
            "identity_attack": 9.263094398193061e-05,
            "insult": 0.0001668655313551426,
            "threat": 3.769186878344044e-05,
            "sexual_explicit": 2.500762275303714e-05
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": {
            "name": [
              "+1",
              "_skip_labeling"
            ],
            "count": [
              3,
              1
            ]
          },
          "labels": {
            "name": [
              "spam",
              "fails_task",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "helpfulness",
              "creativity",
              "violence"
            ],
            "value": [
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.9166666666666666,
              0.375,
              0.375,
              0.75,
              0.375,
              0.0
            ],
            "count": [
              3,
              2,
              3,
              2,
              2,
              2,
              2,
              3,
              2,
              2,
              2,
              2,
              2
            ]
          }
        },
        "truncated_cells": []
      },
      {
        "row_idx": 2,
        "row": {
          "message_id": "6708c47f-05c9-4346-b3d2-40b2bd24fde4",
          "parent_id": "c8e83833-ecbc-44fe-b6db-735228c25a1c",
          "user_id": "2c96e467-66f0-4be7-9693-bda51356a424",
          "created_date": "2023-02-06T18:48:49.391686+00:00",
          "text": "Now explain it to a dog",
          "role": "prompter",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": null,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.03648477792739868,
            "severe_toxicity": 5.486844383995049e-05,
            "obscene": 0.0003762090636882931,
            "identity_attack": 0.0002415566414128989,
            "insult": 0.013612336479127407,
            "threat": 0.0017075861105695367,
            "sexual_explicit": 0.00010235361696686596
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": null,
          "labels": {
            "name": [
              "spam",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "creativity",
              "violence"
            ],
            "value": [
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.25,
              0.5,
              0.625,
              0.5,
              0.0
            ],
            "count": [
              3,
              3,
              2,
              2,
              2,
              2,
              3,
              2,
              2,
              2,
              2
            ]
          }
        },
        "truncated_cells": []
      },
      {
        "row_idx": 3,
        "row": {
          "message_id": "343ee2d4-87ae-41fd-a768-bdd65959dc4a",
          "parent_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "user_id": "49ddcb0d-6588-43bd-858d-19c577f12e7b",
          "created_date": "2023-02-06T13:37:56.044680+00:00",
          "text": "Monopsony is a market structure in which there is a single buyer in a market. In the context of labor markets, a monopsony occurs when there is only one employer in a particular industry or geographic area, giving that employer significant power over the wages and working conditions of workers. This results in a situation where the employer has the ability to suppress wages and reduce the bargaining power of workers.\n\nResearch has shown that the existence of monopsony power in labor markets can have negative effects on workers, such as lower wages, reduced benefits, and limited job opportunities. For example, a study by Autor, Dorn, and Hanson (2013) found evidence of monopsony power in certain U.S. labor markets, resulting in lower wages and reduced job opportunities for workers. Similarly, another study by Bharadwaj, Black, and Kolesnikova (2019) found that the presence of a single dominant employer in a local labor market was associated with lower wages for workers.",
          "role": "assistant",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": 1,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.0008866374846547842,
            "severe_toxicity": 7.99110930529423e-05,
            "obscene": 0.00034686794970184565,
            "identity_attack": 0.0004464525554794818,
            "insult": 0.0005109519697725773,
            "threat": 8.753630390856415e-05,
            "sexual_explicit": 5.744404188590124e-05
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": {
            "name": [
              "+1",
              "_skip_reply",
              "_skip_labeling"
            ],
            "count": [
              2,
              1,
              2
            ]
          },
          "labels": {
            "name": [
              "spam",
              "fails_task",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "helpfulness",
              "creativity",
              "violence"
            ],
            "value": [
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.8333333333333334,
              0.25,
              0.16666666666666666,
              0.9166666666666666,
              0.25,
              0.16666666666666666
            ],
            "count": [
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3
            ]
          }
        },
        "truncated_cells": []
      },
      {
        "row_idx": 4,
        "row": {
          "message_id": "18145bf4-37fd-4ac0-80f5-6108b5f2b365",
          "parent_id": "343ee2d4-87ae-41fd-a768-bdd65959dc4a",
          "user_id": "e10e99a0-38ac-4b07-bf5d-4427696e4e0d",
          "created_date": "2023-02-06T18:52:51.428543+00:00",
          "text": "How can one fight back when a monospony had been created?",
          "role": "prompter",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": null,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.0009362137061543763,
            "severe_toxicity": 6.723815022269264e-05,
            "obscene": 0.00041612397762946784,
            "identity_attack": 0.00018043046293314546,
            "insult": 0.00034414001856930554,
            "threat": 0.00023999605036806315,
            "sexual_explicit": 6.387894973158836e-05
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": {
            "name": [
              "+1"
            ],
            "count": [
              1
            ]
          },
          "labels": {
            "name": [
              "spam",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "creativity",
              "violence"
            ],
            "value": [
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.75,
              0.0,
              0.0,
              0.0,
              0.0
            ],
            "count": [
              3,
              3,
              2,
              2,
              2,
              2,
              3,
              2,
              2,
              2,
              2
            ]
          }
        },
        "truncated_cells": []
      },
      {
        "row_idx": 5,
        "row": {
          "message_id": "636dd191-50df-4894-ba9a-cd7f00767258",
          "parent_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "user_id": "c212120c-0b79-4b32-a5b1-d96686ca92ca",
          "created_date": "2023-02-06T14:28:09.611612+00:00",
          "text": "Monopsony refers to a market structure where there is only one buyer of a good or service. In the context of labor markets, a monopsony occurs when there is only one employer in a particular geographic area or industry, giving them significant power to set wages and working conditions. Research has shown that the existence of monopsonies can lead to lower wages and reduced job opportunities for workers. For example, a study by the National Bureau of Economic Research found that in industries with high levels of concentration, workers earn lower wages and are less likely to receive benefits such as health insurance.",
          "role": "assistant",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": 2,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.0002960403508041054,
            "severe_toxicity": 3.159583866363391e-05,
            "obscene": 0.0001712603261694312,
            "identity_attack": 0.00010074262536363676,
            "insult": 0.00018406218441668898,
            "threat": 4.080198414158076e-05,
            "sexual_explicit": 2.7802774638985284e-05
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": {
            "name": [
              "+1",
              "_skip_reply"
            ],
            "count": [
              6,
              3
            ]
          },
          "labels": {
            "name": [
              "spam",
              "fails_task",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "helpfulness",
              "creativity",
              "violence"
            ],
            "value": [
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.8333333333333334,
              0.0,
              0.0,
              0.8333333333333334,
              0.0,
              0.0
            ],
            "count": [
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3
            ]
          }
        },
        "truncated_cells": []
      },
      {
        "row_idx": 6,
        "row": {
          "message_id": "ac94bfcf-7f25-4084-8755-dde345ac2323",
          "parent_id": "636dd191-50df-4894-ba9a-cd7f00767258",
          "user_id": "3ef645f1-c9f7-49e0-8e53-3c9a9c440ca1",
          "created_date": "2023-02-06T16:44:30.360408+00:00",
          "text": "What can be done at a regulatory level to ensure, in a monopsony, that the power over the employees isn't abused? List a few options and focus on the organs that should act.",
          "role": "prompter",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": null,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.0062903789803385735,
            "severe_toxicity": 9.325604332843795e-05,
            "obscene": 0.0004946678527630866,
            "identity_attack": 0.00021326237765606493,
            "insult": 0.0005733256111852825,
            "threat": 0.00121682311873883,
            "sexual_explicit": 0.0007907780236564577
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": {
            "name": [
              "+1",
              "-1",
              "_skip_reply",
              "_skip_ranking",
              "_skip_labeling"
            ],
            "count": [
              3,
              1,
              9,
              5,
              1
            ]
          },
          "labels": {
            "name": [
              "spam",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "creativity",
              "violence"
            ],
            "value": [
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.6666666666666666,
              0.125,
              0.0,
              0.375,
              0.0
            ],
            "count": [
              3,
              3,
              2,
              2,
              2,
              2,
              3,
              2,
              2,
              2,
              2
            ]
          }
        },
        "truncated_cells": []
      },
      {
        "row_idx": 7,
        "row": {
          "message_id": "73d6f715-3787-409c-81e4-fde0e5ef60cd",
          "parent_id": "ac94bfcf-7f25-4084-8755-dde345ac2323",
          "user_id": "2c96e467-66f0-4be7-9693-bda51356a424",
          "created_date": "2023-02-06T18:42:19.103071+00:00",
          "text": "Here are some potential regulatory options to address monopsony power over employees:\n\n- Antitrust enforcement agencies like the FTC or DOJ could investigate and take action against dominant employers that are abusing their market power to suppress wages or worsen working conditions. They could force divestitures or other structural remedies.\n- Labor standards agencies could set industry-specific minimum wage floors or other regulations around compensation and working conditions. This would reduce the ability of dominant employers to push wages/conditions below certain thresholds.\n- Sector-specific regulators could impose requirements around how companies hire, compensate, and treat workers. For example, banking regulators could prohibit abusive practices by dominant banks with regard to their employees.\n- Legislatures could pass laws",
          "role": "assistant",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": 0,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.00036603500484488904,
            "severe_toxicity": 2.5196904971380718e-05,
            "obscene": 0.00010540795483393595,
            "identity_attack": 0.00011445347627159208,
            "insult": 0.00016151837189681828,
            "threat": 3.965371070080437e-05,
            "sexual_explicit": 2.380239857302513e-05
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": {
            "name": [
              "_skip_labeling"
            ],
            "count": [
              1
            ]
          },
          "labels": {
            "name": [
              "spam",
              "fails_task",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "helpfulness",
              "creativity",
              "violence"
            ],
            "value": [
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.5833333333333334,
              0.125,
              0.0,
              0.375,
              0.0,
              0.0
            ],
            "count": [
              3,
              2,
              3,
              2,
              2,
              2,
              2,
              3,
              2,
              2,
              2,
              2,
              2
            ]
          }
        },
        "truncated_cells": []
      },
      {
        "row_idx": 8,
        "row": {
          "message_id": "b280ccbc-b68f-42b9-9fc2-d7ac89b88022",
          "parent_id": "ac94bfcf-7f25-4084-8755-dde345ac2323",
          "user_id": "cd4e0925-f3f0-478e-b6d5-a9fd66a614cb",
          "created_date": "2023-02-06T19:47:32.808838+00:00",
          "text": "Regulatory intervention can be used to address the negative consequences of monopsony power in the labor market. A few options include:\n\n1. Antitrust enforcement: Antitrust laws can be used to prevent firms from engaging in practices that limit competition and suppress wages. The antitrust division of the Department of Justice and the Federal Trade Commission (FTC) are responsible for enforcing these laws.\n\n2. Labor standards and protections: Governments can set minimum wage standards and other labor protections to ensure that workers are paid a fair wage, even in industries dominated by a single buyer. The U.S. Department of Labor is responsible for enforcing these standards.\n\n3. Occupational licensing reform: Occupational licensing requirements can restrict worker mobility and increase the bargaining power of employers. Reforms to licensing requirements can reduce these barriers and increase competition in the labor market. State governments typically regulate occupational licensing.\n\n4. Collective bargaining rights: Collective bargaining by workers can increase their bargaining power and lead to higher wages. Governments can protect and promote collective bargaining rights to counteract the effects of monopsony power. The National Labor Relations Board is responsible for enforcing workers' rights to engage in collective bargaining in the U.S.\n\nThese are just a few examples of regulatory options that can be used to address monopsony power in the labor market. The specific actions taken will depend on the particular circumstances and context of each case.",
          "role": "assistant",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": 1,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.00031410896917805076,
            "severe_toxicity": 2.3768236133037135e-05,
            "obscene": 0.00011176631960552186,
            "identity_attack": 9.1741414507851e-05,
            "insult": 0.00014727565576322377,
            "threat": 3.7340509152272716e-05,
            "sexual_explicit": 2.486484299879521e-05
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": null,
          "labels": {
            "name": [
              "spam",
              "fails_task",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "helpfulness",
              "creativity",
              "violence"
            ],
            "value": [
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.0,
              0.25,
              0.0,
              0.0,
              0.25,
              0.0,
              0.0
            ],
            "count": [
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3,
              3
            ]
          }
        },
        "truncated_cells": []
      },
      {
        "row_idx": 9,
        "row": {
          "message_id": "be4d0752-afbe-473b-99f3-48f1b5880f90",
          "parent_id": "ac94bfcf-7f25-4084-8755-dde345ac2323",
          "user_id": "c212120c-0b79-4b32-a5b1-d96686ca92ca",
          "created_date": "2023-02-06T18:34:30.836581+00:00",
          "text": "Yes, that's correct. Keeping the code for the TESR and the registration code in the same file can make it easier to manage and maintain the code. The ExampleMod class acts as a container for the TESR code, providing a convenient and logical grouping. The functionality of the code remains unchanged, and the TESR will still be properly registered and used to render the custom tile entities in the game.",
          "role": "assistant",
          "lang": "en",
          "review_count": 3,
          "review_result": true,
          "deleted": false,
          "rank": 2,
          "synthetic": false,
          "model_name": null,
          "detoxify": {
            "toxicity": 0.00026471694582141936,
            "severe_toxicity": 3.753640703507699e-05,
            "obscene": 0.00018712480959948152,
            "identity_attack": 0.00012360737309791148,
            "insult": 0.000221485854126513,
            "threat": 4.4919768697582185e-05,
            "sexual_explicit": 3.185281457263045e-05
          },
          "message_tree_id": "6ab24d72-0181-4594-a9cd-deaf170242fb",
          "tree_state": "ready_for_export",
          "emojis": {
            "name": [
              "_skip_labeling"
            ],
            "count": [
              4
            ]
          },
          "labels": {
            "name": [
              "spam",
              "fails_task",
              "lang_mismatch",
              "pii",
              "not_appropriate",
              "hate_speech",
              "sexual_content",
              "quality",
              "toxicity",
              "humor",
              "helpfulness",
              "creativity",
              "violence"
            ],
            "value": [
              0.3333333333333333,
              1.0,
              0.0,
              0.0,
              0.5,
              0.0,
              0.0,
              0.16666666666666666,
              0.0,
              0.125,
              0.0,
              0.25,
              0.0
            ],
            "count": [
              3,
              2,
              3,
              2,
              2,
              2,
              2,
              3,
              2,
              2,
              2,
              2,
              2
            ]
          }
        },
        "truncated_cells": []
      }
    ],
    "modalities": [
      "Tabular",
      "Text"
    ]
  },
  {
    "_id": "639244f571c51c43091df168",
    "id": "Anthropic/hh-rlhf",
    "author": "Anthropic",
    "disabled": false,
    "gated": false,
    "lastModified": "2023-05-26T18:47:34+00:00",
    "likes": 1344,
    "private": false,
    "sha": "09be8c5bbc57cb3887f3a9732ad6aa7ec602a1fa",
    "description": "\n\t\n\t\t\n\t\tDataset Card for HH-RLHF\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to two different kinds of data:\n\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead… See the full description on the dataset page: https://huggingface.co/datasets/Anthropic/hh-rlhf.",
    "downloads": 12573,
    "tags": [
      "license:mit",
      "size_categories:100K<n<1M",
      "format:json",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2204.05862",
      "region:us",
      "human-feedback"
    ],
    "createdAt": "2022-12-08T20:11:33+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "63da45beaa68107243466309",
    "id": "gsdf/EasyNegative",
    "author": "gsdf",
    "disabled": false,
    "gated": false,
    "lastModified": "2023-02-12T14:39:30+00:00",
    "likes": 1149,
    "private": false,
    "sha": "60067b257337df8d7879142d870944fe4c6ab20d",
    "description": "\n\t\n\t\t\n\t\tNegative Embedding\n\t\n\nThis is a Negative Embedding trained with Counterfeit. Please use it in the \"\\stable-diffusion-webui\\embeddings\" folder.It can be used with other models, but the effectiveness is not certain.  \n\n\t\n\t\t\n\t\tCounterfeit-V2.0.safetensors\n\t\n\n\n\n\t\n\t\t\n\t\tAbyssOrangeMix2_sfw.safetensors\n\t\n\n\n\n\t\n\t\t\n\t\tanything-v4.0-pruned.safetensors\n\t\n\n\n",
    "downloads": 37605,
    "tags": [
      "license:other",
      "size_categories:n<1K",
      "format:imagefolder",
      "modality:image",
      "library:datasets",
      "library:mlcroissant",
      "region:us"
    ],
    "createdAt": "2023-02-01T10:58:06+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Image"
    ]
  },
  {
    "_id": "643ce713099590e9ed8f29f7",
    "id": "togethercomputer/RedPajama-Data-1T",
    "author": "togethercomputer",
    "disabled": false,
    "gated": false,
    "lastModified": "2024-06-17T11:36:03+00:00",
    "likes": 1090,
    "private": false,
    "sha": "398f92572e94f4793e41c22ab7ea2a788d9e7de4",
    "description": "RedPajama is a clean-room, fully open-source implementation of the LLaMa dataset.",
    "downloads": 2848,
    "tags": [
      "task_categories:text-generation",
      "language:en",
      "size_categories:1M<n<10M",
      "modality:text",
      "library:datasets",
      "library:mlcroissant",
      "region:us"
    ],
    "createdAt": "2023-04-17T06:28:35+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "63769dcacb2e84c60b0a1905",
    "id": "Nerfgun3/bad_prompt",
    "author": "Nerfgun3",
    "disabled": false,
    "gated": false,
    "lastModified": "2022-11-19T23:43:47+00:00",
    "likes": 925,
    "private": false,
    "sha": "26a7b45850bfdafeda574d1bc79b2f16700748e1",
    "description": "\n\t\n\t\t\n\t\tNegative Embedding / Textual Inversion\n\t\n\n\n\n\n\t\n\t\t\n\t\tIdea\n\t\n\nThe idea behind this embedding was to somehow train the negative prompt as an embedding, thus unifying the basis of the negative prompt into one word or embedding. \nSide note: Embedding has proven to be very helpful for the generation of hands! :)\n\n\t\n\t\t\n\t\tUsage\n\t\n\nTo use this embedding you have to download the file aswell as drop it into the \"\\stable-diffusion-webui\\embeddings\" folder.\nPlease put the embedding in the negative… See the full description on the dataset page: https://huggingface.co/datasets/Nerfgun3/bad_prompt.",
    "downloads": 4482,
    "tags": [
      "language:en",
      "license:creativeml-openrail-m",
      "size_categories:n<1K",
      "format:imagefolder",
      "modality:image",
      "library:datasets",
      "library:mlcroissant",
      "region:us",
      "stable-diffusion",
      "text-to-image",
      "image-to-image"
    ],
    "createdAt": "2022-11-17T20:47:06+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Image"
    ]
  },
  {
    "_id": "649f37af37bfb5202beabdf4",
    "id": "allenai/dolma",
    "author": "allenai",
    "disabled": false,
    "gated": false,
    "lastModified": "2024-04-17T02:57:00+00:00",
    "likes": 905,
    "private": false,
    "sha": "7f48140530a023e9ea4c5cfb141160922727d4d3",
    "citation": "@article{dolma,\n  title = {{Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}},\n  author = {\n    Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and\n    Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and\n    Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Ian Magnusson and\n    Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and\n    Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and\n    Oyvind Tafjord and Evan Pete Walsh and Hannaneh Hajishirzi and Noah A. Smith and Luke Zettlemoyer and\n    Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo\n},\n  year = {2024},\n  journal={arXiv preprint},\n}",
    "description": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research",
    "downloads": 1812,
    "tags": [
      "task_categories:text-generation",
      "language:en",
      "license:odc-by",
      "size_categories:n>1T",
      "arxiv:2402.00159",
      "arxiv:2301.13688",
      "region:us",
      "language-modeling",
      "casual-lm",
      "llm"
    ],
    "createdAt": "2023-06-30T20:14:39+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": []
  },
  {
    "_id": "6457bc5798a8724fa6120362",
    "id": "tiiuae/falcon-refinedweb",
    "author": "tiiuae",
    "disabled": false,
    "gated": false,
    "lastModified": "2023-06-20T12:38:07+00:00",
    "likes": 854,
    "private": false,
    "sha": "c735840575b629292b41da8dde11dcd523d4f91c",
    "description": "\n\t\n\t\t\n\t\t📀 Falcon RefinedWeb\n\t\n\nFalcon RefinedWeb is a massive English web dataset built by TII and released under an ODC-By 1.0 license.\nSee the 📓 paper on arXiv for more details. \nRefinedWeb is built through stringent filtering and large-scale deduplication of CommonCrawl; we found models trained on RefinedWeb to achieve performance in-line or better than models trained on curated datasets, while only relying on web data. \nRefinedWeb is also \"multimodal-friendly\": it contains links and alt… See the full description on the dataset page: https://huggingface.co/datasets/tiiuae/falcon-refinedweb.",
    "downloads": 17426,
    "tags": [
      "task_categories:text-generation",
      "language:en",
      "license:odc-by",
      "size_categories:100M<n<1B",
      "format:parquet",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2306.01116",
      "arxiv:2203.15556",
      "arxiv:2107.06499",
      "arxiv:2104.08758",
      "arxiv:2109.07445",
      "arxiv:1911.00359",
      "arxiv:2112.11446",
      "doi:10.57967/hf/0737",
      "region:us"
    ],
    "createdAt": "2023-05-07T14:57:27+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "621ffdd236468d709f184284",
    "id": "wikimedia/wikipedia",
    "author": "wikimedia",
    "disabled": false,
    "gated": false,
    "lastModified": "2024-01-09T09:40:51+00:00",
    "likes": 830,
    "private": false,
    "sha": "b04c8d1ceb2f5cd4588862100d08de323dccfbaa",
    "description": "\n\t\n\t\t\n\t\tDataset Card for Wikimedia Wikipedia\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWikipedia dataset containing cleaned articles of all languages.\nThe dataset is built from the Wikipedia dumps (https://dumps.wikimedia.org/)\nwith one subset per language, each containing a single train split.\nEach example contains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).\nAll language subsets have already been processed for recent dump, and you… See the full description on the dataset page: https://huggingface.co/datasets/wikimedia/wikipedia.",
    "downloads": 79914,
    "tags": [
      "task_categories:text-generation",
      "task_categories:fill-mask",
      "task_ids:language-modeling",
      "task_ids:masked-language-modeling",
      "language:ab",
      "language:ace",
      "language:ady",
      "language:af",
      "language:alt",
      "language:am",
      "language:ami",
      "language:an",
      "language:ang",
      "language:anp",
      "language:ar",
      "language:arc",
      "language:ary",
      "language:arz",
      "language:as",
      "language:ast",
      "language:atj",
      "language:av",
      "language:avk",
      "language:awa",
      "language:ay",
      "language:az",
      "language:azb",
      "language:ba",
      "language:ban",
      "language:bar",
      "language:bbc",
      "language:bcl",
      "language:be",
      "language:bg",
      "language:bh",
      "language:bi",
      "language:bjn",
      "language:blk",
      "language:bm",
      "language:bn",
      "language:bo",
      "language:bpy",
      "language:br",
      "language:bs",
      "language:bug",
      "language:bxr",
      "language:ca",
      "language:cbk",
      "language:cdo",
      "language:ce",
      "language:ceb",
      "language:ch",
      "language:chr",
      "language:chy",
      "language:ckb",
      "language:co",
      "language:cr",
      "language:crh",
      "language:cs",
      "language:csb",
      "language:cu",
      "language:cv",
      "language:cy",
      "language:da",
      "language:dag",
      "language:de",
      "language:dga",
      "language:din",
      "language:diq",
      "language:dsb",
      "language:dty",
      "language:dv",
      "language:dz",
      "language:ee",
      "language:el",
      "language:eml",
      "language:en",
      "language:eo",
      "language:es",
      "language:et",
      "language:eu",
      "language:ext",
      "language:fa",
      "language:fat",
      "language:ff",
      "language:fi",
      "language:fj",
      "language:fo",
      "language:fon",
      "language:fr",
      "language:frp",
      "language:frr",
      "language:fur",
      "language:fy",
      "language:ga",
      "language:gag",
      "language:gan",
      "language:gcr",
      "language:gd",
      "language:gl",
      "language:glk",
      "language:gn",
      "language:gom",
      "language:gor",
      "language:got",
      "language:gpe",
      "language:gsw",
      "language:gu",
      "language:guc",
      "language:gur",
      "language:guw",
      "language:gv",
      "language:ha",
      "language:hak",
      "language:haw",
      "language:hbs",
      "language:he",
      "language:hi",
      "language:hif",
      "language:hr",
      "language:hsb",
      "language:ht",
      "language:hu",
      "language:hy",
      "language:hyw",
      "language:ia",
      "language:id",
      "language:ie",
      "language:ig",
      "language:ik",
      "language:ilo",
      "language:inh",
      "language:io",
      "language:is",
      "language:it",
      "language:iu",
      "language:ja",
      "language:jam",
      "language:jbo",
      "language:jv",
      "language:ka",
      "language:kaa",
      "language:kab",
      "language:kbd",
      "language:kbp",
      "language:kcg",
      "language:kg",
      "language:ki",
      "language:kk",
      "language:kl",
      "language:km",
      "language:kn",
      "language:ko",
      "language:koi",
      "language:krc",
      "language:ks",
      "language:ksh",
      "language:ku",
      "language:kv",
      "language:kw",
      "language:ky",
      "language:la",
      "language:lad",
      "language:lb",
      "language:lbe",
      "language:lez",
      "language:lfn",
      "language:lg",
      "language:li",
      "language:lij",
      "language:lld",
      "language:lmo",
      "language:ln",
      "language:lo",
      "language:lt",
      "language:ltg",
      "language:lv",
      "language:lzh",
      "language:mad",
      "language:mai",
      "language:map",
      "language:mdf",
      "language:mg",
      "language:mhr",
      "language:mi",
      "language:min",
      "language:mk",
      "language:ml",
      "language:mn",
      "language:mni",
      "language:mnw",
      "language:mr",
      "language:mrj",
      "language:ms",
      "language:mt",
      "language:mwl",
      "language:my",
      "language:myv",
      "language:mzn",
      "language:nah",
      "language:nan",
      "language:nap",
      "language:nds",
      "language:ne",
      "language:new",
      "language:nia",
      "language:nl",
      "language:nn",
      "language:no",
      "language:nov",
      "language:nqo",
      "language:nrf",
      "language:nso",
      "language:nv",
      "language:ny",
      "language:oc",
      "language:olo",
      "language:om",
      "language:or",
      "language:os",
      "language:pa",
      "language:pag",
      "language:pam",
      "language:pap",
      "language:pcd",
      "language:pcm",
      "language:pdc",
      "language:pfl",
      "language:pi",
      "language:pih",
      "language:pl",
      "language:pms",
      "language:pnb",
      "language:pnt",
      "language:ps",
      "language:pt",
      "language:pwn",
      "language:qu",
      "language:rm",
      "language:rmy",
      "language:rn",
      "language:ro",
      "language:ru",
      "language:rue",
      "language:rup",
      "language:rw",
      "language:sa",
      "language:sah",
      "language:sat",
      "language:sc",
      "language:scn",
      "language:sco",
      "language:sd",
      "language:se",
      "language:sg",
      "language:sgs",
      "language:shi",
      "language:shn",
      "language:si",
      "language:sk",
      "language:skr",
      "language:sl",
      "language:sm",
      "language:smn",
      "language:sn",
      "language:so",
      "language:sq",
      "language:sr",
      "language:srn",
      "language:ss",
      "language:st",
      "language:stq",
      "language:su",
      "language:sv",
      "language:sw",
      "language:szl",
      "language:szy",
      "language:ta",
      "language:tay",
      "language:tcy",
      "language:te",
      "language:tet",
      "language:tg",
      "language:th",
      "language:ti",
      "language:tk",
      "language:tl",
      "language:tly",
      "language:tn",
      "language:to",
      "language:tpi",
      "language:tr",
      "language:trv",
      "language:ts",
      "language:tt",
      "language:tum",
      "language:tw",
      "language:ty",
      "language:tyv",
      "language:udm",
      "language:ug",
      "language:uk",
      "language:ur",
      "language:uz",
      "language:ve",
      "language:vec",
      "language:vep",
      "language:vi",
      "language:vls",
      "language:vo",
      "language:vro",
      "language:wa",
      "language:war",
      "language:wo",
      "language:wuu",
      "language:xal",
      "language:xh",
      "language:xmf",
      "language:yi",
      "language:yo",
      "language:yue",
      "language:za",
      "language:zea",
      "language:zgh",
      "language:zh",
      "language:zu",
      "license:cc-by-sa-3.0",
      "license:gfdl",
      "size_categories:10M<n<100M",
      "format:parquet",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "region:us"
    ],
    "createdAt": "2022-03-02T23:29:22+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "64358e2179c45fcf1ada09f4",
    "id": "databricks/databricks-dolly-15k",
    "author": "databricks",
    "disabled": false,
    "gated": false,
    "lastModified": "2023-06-30T18:34:13+00:00",
    "likes": 820,
    "private": false,
    "sha": "bdd27f4d94b9c1f951818a7da7fd7aeea5dbff1a",
    "description": "\n\t\n\t\t\n\t\tSummary\n\t\n\ndatabricks-dolly-15k is an open source dataset of instruction-following records generated by thousands of Databricks employees in several \nof the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, \nclosed QA, generation, information extraction, open QA, and summarization.\nThis dataset can be used for any purpose, whether academic or commercial,  under the terms of the \nCreative Commons Attribution-ShareAlike 3.0 Unported… See the full description on the dataset page: https://huggingface.co/datasets/databricks/databricks-dolly-15k.",
    "downloads": 17357,
    "tags": [
      "task_categories:question-answering",
      "task_categories:summarization",
      "language:en",
      "license:cc-by-sa-3.0",
      "size_categories:10K<n<100K",
      "format:json",
      "modality:text",
      "library:datasets",
      "library:pandas",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2203.02155",
      "region:us"
    ],
    "createdAt": "2023-04-11T16:43:13+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "633a585e593f7e38374056ec",
    "id": "bigcode/the-stack",
    "author": "bigcode",
    "disabled": false,
    "gated": "auto",
    "lastModified": "2023-04-13T12:15:50+00:00",
    "likes": 814,
    "private": false,
    "sha": "349a71353fd5868fb90b593ef09e311379da498a",
    "description": "\n\t\n\t\t\n\t\tDataset Card for The Stack\n\t\n\n\n\n\t\n\t\t\n\t\tChangelog\n\t\n\n\n\t\n\t\t\nRelease\nDescription\n\n\n\t\t\nv1.0\nInitial release of the Stack. Included 30 programming languages and 18 permissive licenses. Note: Three included licenses (MPL/EPL/LGPL) are considered weak copyleft licenses. The resulting near-deduplicated dataset is 3TB in size.\n\n\nv1.1\nThe three copyleft licenses ((MPL/EPL/LGPL) were excluded and the list of permissive licenses extended to 193 licenses in total. The list of programming languages… See the full description on the dataset page: https://huggingface.co/datasets/bigcode/the-stack.",
    "downloads": 24274,
    "tags": [
      "task_categories:text-generation",
      "language_creators:crowdsourced",
      "language_creators:expert-generated",
      "multilinguality:multilingual",
      "language:code",
      "license:other",
      "size_categories:100M<n<1B",
      "format:parquet",
      "modality:tabular",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2211.15533",
      "arxiv:2107.03374",
      "arxiv:2207.14157",
      "region:us"
    ],
    "createdAt": "2022-10-03T03:34:54+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Tabular",
      "Text"
    ]
  },
  {
    "_id": "642912f7a760fe0bf37996b1",
    "id": "anon8231489123/ShareGPT_Vicuna_unfiltered",
    "author": "anon8231489123",
    "disabled": false,
    "gated": false,
    "lastModified": "2023-04-12T05:23:59+00:00",
    "likes": 793,
    "private": false,
    "sha": "192ab2185289094fc556ec8ce5ce1e8e587154ca",
    "description": "Further cleaning done. Please look through the dataset and ensure that I didn't miss anything.\nUpdate: Confirmed working method for training the model: https://huggingface.co/AlekseyKorshuk/vicuna-7b/discussions/4#64346c08ef6d5abefe42c12c\nTwo choices:\n\nRemoves instances of \"I'm sorry, but\": https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json\nHas instances of \"I'm sorry, but\":… See the full description on the dataset page: https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered.",
    "downloads": 39499,
    "tags": [
      "language:en",
      "license:apache-2.0",
      "region:us"
    ],
    "createdAt": "2023-04-02T05:30:31+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": []
  },
  {
    "_id": "640f5b2fb63b6f18522d6d44",
    "id": "tatsu-lab/alpaca",
    "author": "tatsu-lab",
    "disabled": false,
    "gated": false,
    "lastModified": "2023-05-22T20:33:36+00:00",
    "likes": 762,
    "private": false,
    "sha": "dce01c9b08f87459cf36a430d809084718273017",
    "description": "\n\t\n\t\t\n\t\tDataset Card for Alpaca\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAlpaca is a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better.\nThe authors built on the data generation pipeline from Self-Instruct framework and made the following modifications:\n\nThe text-davinci-003 engine to generate the instruction data instead… See the full description on the dataset page: https://huggingface.co/datasets/tatsu-lab/alpaca.",
    "downloads": 40459,
    "tags": [
      "task_categories:text-generation",
      "language:en",
      "license:cc-by-nc-4.0",
      "size_categories:10K<n<100K",
      "format:parquet",
      "modality:text",
      "library:datasets",
      "library:pandas",
      "library:mlcroissant",
      "library:polars",
      "region:us",
      "instruction-finetuning"
    ],
    "createdAt": "2023-03-13T17:19:43+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "625552d2b339bb03abe3432d",
    "id": "openai/gsm8k",
    "author": "openai",
    "disabled": false,
    "gated": false,
    "lastModified": "2024-01-04T12:05:15+00:00",
    "likes": 747,
    "private": false,
    "sha": "e53f048856ff4f594e959d75785d2c2d37b678ee",
    "description": "\n\t\n\t\t\n\t\tDataset Card for GSM8K\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.\n\nThese problems take between 2 and 8 steps to solve.\nSolutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − ×÷) to reach the… See the full description on the dataset page: https://huggingface.co/datasets/openai/gsm8k.",
    "downloads": 545923,
    "paperswithcode_id": "gsm8k",
    "tags": [
      "task_categories:text2text-generation",
      "annotations_creators:crowdsourced",
      "language_creators:crowdsourced",
      "multilinguality:monolingual",
      "source_datasets:original",
      "language:en",
      "license:mit",
      "size_categories:10K<n<100K",
      "format:parquet",
      "modality:text",
      "library:datasets",
      "library:pandas",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2110.14168",
      "region:us",
      "math-word-problems"
    ],
    "createdAt": "2022-04-12T10:22:10+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "676f70846bf205795346d2be",
    "id": "FreedomIntelligence/medical-o1-reasoning-SFT",
    "author": "FreedomIntelligence",
    "disabled": false,
    "gated": false,
    "lastModified": "2025-04-22T15:11:21+00:00",
    "likes": 735,
    "private": false,
    "sha": "fc2c9e8a37b38f38da6d449564a8c350b244aef4",
    "description": "\n\t\n\t\t\n\t\tNews\n\t\n\n[2025/04/22] We split the data and kept only the medical SFT dataset (medical_o1_sft.json). The file medical_o1_sft_mix.json contains a mix of medical and general instruction data.\n[2025/02/22] We released the distilled dataset from Deepseek-R1 based on medical verifiable problems. You can use it to initialize your models with the reasoning chain from Deepseek-R1.\n[2024/12/25] We open-sourced the medical reasoning dataset for SFT, built on medical verifiable problems and an LLM… See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT.",
    "downloads": 11823,
    "tags": [
      "task_categories:question-answering",
      "task_categories:text-generation",
      "language:en",
      "language:zh",
      "license:apache-2.0",
      "size_categories:10K<n<100K",
      "format:json",
      "modality:text",
      "library:datasets",
      "library:pandas",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2412.18925",
      "region:us",
      "medical",
      "biology"
    ],
    "createdAt": "2024-12-28T03:29:08+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "641f0c169c1013836ce2e5eb",
    "id": "QingyiSi/Alpaca-CoT",
    "author": "QingyiSi",
    "disabled": false,
    "gated": false,
    "lastModified": "2023-09-14T08:52:10+00:00",
    "likes": 730,
    "private": false,
    "sha": "18add89e3b884703ec869a5c6e2bcf1412ee7edc",
    "description": "\n\t\n\t\t\n\t\n\t\n\t\tInstruction-Finetuning Dataset Collection (Alpaca-CoT)\n\t\n\nThis repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.\nWe also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  \nIf you think this dataset collection is helpful to you, please like… See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT.",
    "downloads": 14403,
    "tags": [
      "language:en",
      "language:zh",
      "language:ml",
      "license:apache-2.0",
      "region:us",
      "Instruction",
      "Cot"
    ],
    "createdAt": "2023-03-25T14:58:30+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": []
  },
  {
    "_id": "655100ea2adb0688a0042ddd",
    "id": "teknium/OpenHermes-2.5",
    "author": "teknium",
    "disabled": false,
    "gated": false,
    "lastModified": "2024-04-15T08:18:12+00:00",
    "likes": 729,
    "private": false,
    "sha": "b82037821055c377bed0d495e72e46de3bc72e84",
    "description": "\n\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\nThis is the dataset that made OpenHermes 2.5 and Nous Hermes 2 series of models.\nSupport me on GitHub sponsors <3 : https://github.com/sponsors/teknium1\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Open Hermes 2/2.5 and Nous Hermes 2 models have made significant advancements of SOTA LLM's over recent months, and are underpinned by this exact compilation and curation of many open source datasets and custom created synthetic datasets.… See the full description on the dataset page: https://huggingface.co/datasets/teknium/OpenHermes-2.5.",
    "downloads": 4278,
    "tags": [
      "language:eng",
      "size_categories:1M<n<10M",
      "format:json",
      "modality:text",
      "library:datasets",
      "library:pandas",
      "library:mlcroissant",
      "library:polars",
      "region:us",
      "synthetic",
      "GPT-4",
      "Distillation",
      "Compilation"
    ],
    "createdAt": "2023-11-12T16:44:26+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "6797e648de960c48ff034e54",
    "id": "open-thoughts/OpenThoughts-114k",
    "author": "open-thoughts",
    "disabled": false,
    "gated": false,
    "lastModified": "2025-04-06T23:31:24+00:00",
    "likes": 707,
    "private": false,
    "sha": "a5996b0064b4ddd42c6e9a7302eeec0618cb7b63",
    "description": "\n    \n\n\n\n \n\n\n\n\t\n\t\t\n\t\tOpen-Thoughts-114k\n\t\n\nOpen synthetic reasoning dataset with 114k high-quality examples covering math, science, code, and puzzles!\nInspect the content with rich formatting with Curator Viewer.\n\n\t\n\t\t\n\t\tAvailable Subsets\n\t\n\ndefault subset containing ready-to-train data used to finetune the OpenThinker-7B and OpenThinker-32B models:\nds = load_dataset(\"open-thoughts/OpenThoughts-114k\", split=\"train\")\n\nmetadata subset containing extra columns used in dataset construction:… See the full description on the dataset page: https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k.",
    "downloads": 16598,
    "tags": [
      "license:apache-2.0",
      "size_categories:100K<n<1M",
      "format:parquet",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "region:us",
      "curator",
      "synthetic"
    ],
    "createdAt": "2025-01-27T20:02:16+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "641debae1d05404efd046a4f",
    "id": "yahma/alpaca-cleaned",
    "author": "yahma",
    "disabled": false,
    "gated": false,
    "lastModified": "2023-04-10T20:29:06+00:00",
    "likes": 688,
    "private": false,
    "sha": "12567cabf869d7c92e573c7c783905fc160e9639",
    "description": "\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the… See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned.",
    "downloads": 23106,
    "tags": [
      "task_categories:text-generation",
      "language:en",
      "license:cc-by-4.0",
      "size_categories:10K<n<100K",
      "format:json",
      "modality:text",
      "library:datasets",
      "library:pandas",
      "library:mlcroissant",
      "library:polars",
      "region:us",
      "instruction-finetuning"
    ],
    "createdAt": "2023-03-24T18:27:58+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "6655eb19d17e141dcb546ed5",
    "id": "HuggingFaceFW/fineweb-edu",
    "author": "HuggingFaceFW",
    "disabled": false,
    "gated": false,
    "lastModified": "2025-01-31T15:56:54+00:00",
    "likes": 684,
    "private": false,
    "sha": "4863ab07d7520451e6f73e2912ad8bfee7d97c11",
    "description": "\n\t\n\t\t\n\t\t📚 FineWeb-Edu\n\t\n\n\n    \n\n\n\n1.3 trillion tokens of the finest educational data the 🌐 web has to offer\n\nPaper: https://arxiv.org/abs/2406.17557\n\n\t\n\t\t\n\t\n\t\n\t\tWhat is it?\n\t\n\n📚 FineWeb-Edu  dataset consists of 1.3T tokens  and  5.4T tokens (FineWeb-Edu-score-2) of educational web pages filtered from 🍷 FineWeb dataset. This is the 1.3 trillion version.\nTo enhance FineWeb's quality, we developed an educational quality classifier using annotations generated by LLama3-70B-Instruct. We then… See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu.",
    "downloads": 132752,
    "tags": [
      "task_categories:text-generation",
      "language:en",
      "license:odc-by",
      "size_categories:1B<n<10B",
      "format:parquet",
      "modality:tabular",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2406.17557",
      "arxiv:2404.14219",
      "arxiv:2401.10020",
      "arxiv:2109.07445",
      "doi:10.57967/hf/2497",
      "region:us"
    ],
    "createdAt": "2024-05-28T14:32:57+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Tabular",
      "Text"
    ]
  },
  {
    "_id": "650a9248d26103b6eee3ea7b",
    "id": "lmsys/lmsys-chat-1m",
    "author": "lmsys",
    "disabled": false,
    "gated": "auto",
    "lastModified": "2024-07-27T09:28:42+00:00",
    "likes": 675,
    "private": false,
    "sha": "200748d9d3cddcc9d782887541057aca0b18c5da",
    "description": "\n\t\n\t\t\n\t\tLMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset\n\t\n\nThis dataset contains one million real-world conversations with 25 state-of-the-art LLMs.\nIt is collected from 210K unique IP addresses in the wild on the Vicuna demo and Chatbot Arena website from April to August 2023.\nEach sample includes a conversation ID, model name, conversation text in OpenAI API JSON format, detected language tag, and OpenAI moderation API tag.\nUser consent is obtained through the \"Terms of use\"… See the full description on the dataset page: https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",
    "downloads": 4394,
    "tags": [
      "size_categories:1M<n<10M",
      "format:parquet",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2309.11998",
      "region:us"
    ],
    "createdAt": "2023-09-20T06:33:44+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "645e8da96320b0efe40ade7a",
    "id": "roneneldan/TinyStories",
    "author": "roneneldan",
    "disabled": false,
    "gated": false,
    "lastModified": "2024-08-12T13:27:26+00:00",
    "likes": 670,
    "private": false,
    "sha": "f54c09fd23315a6f9c86f9dc80f725de7d8f9c64",
    "description": "Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\nDescribed in the following paper: https://arxiv.org/abs/2305.07759. \nThe models referred to in the paper were trained on TinyStories-train.txt  (the file tinystories-valid.txt can be used for validation loss). These models can be found on Huggingface, at roneneldan/TinyStories-1M/3M/8M/28M/33M/1Layer-21M.\nAdditional resources:\ntinystories_all_data.tar.gz - contains a superset of… See the full description on the dataset page: https://huggingface.co/datasets/roneneldan/TinyStories.",
    "downloads": 26605,
    "tags": [
      "task_categories:text-generation",
      "language:en",
      "license:cdla-sharing-1.0",
      "size_categories:1M<n<10M",
      "format:parquet",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2305.07759",
      "region:us"
    ],
    "createdAt": "2023-05-12T19:04:09+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "67b32145bac2756ce9a4a0fe",
    "id": "Congliu/Chinese-DeepSeek-R1-Distill-data-110k",
    "author": "Congliu",
    "disabled": false,
    "gated": false,
    "lastModified": "2025-02-21T02:18:08+00:00",
    "likes": 667,
    "private": false,
    "sha": "8520b649430617c2be4490f424d251d09d835ed3",
    "description": "\n\t\n\t\t\n\t\t中文基于满血DeepSeek-R1蒸馏数据集（Chinese-Data-Distill-From-R1）\n\t\n\n\n🤗 Hugging Face   |   🤖 ModelScope    |   🚀 Github    |   📑 Blog\n\n\n注意：提供了直接SFT使用的版本，点击下载。将数据中的思考和答案整合成output字段，大部分SFT代码框架均可直接直接加载训练。\n本数据集为中文开源蒸馏满血R1的数据集，数据集中不仅包含math数据，还包括大量的通用类型数据，总数量为110K。\n为什么开源这个数据？\nR1的效果十分强大，并且基于R1蒸馏数据SFT的小模型也展现出了强大的效果，但检索发现，大部分开源的R1蒸馏数据集均为英文数据集。 同时，R1的报告中展示，蒸馏模型中同时也使用了部分通用场景数据集。\n为了帮助大家更好地复现R1蒸馏模型的效果，特此开源中文数据集。该中文数据集中的数据分布如下：\n\nMath：共计36568个样本，\nExam：共计2432个样本，\nSTEM：共计12648个样本，… See the full description on the dataset page: https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k.",
    "downloads": 1670,
    "tags": [
      "task_categories:text-generation",
      "task_categories:text2text-generation",
      "task_categories:question-answering",
      "language:zh",
      "license:apache-2.0",
      "size_categories:100K<n<1M",
      "format:json",
      "modality:tabular",
      "modality:text",
      "library:datasets",
      "library:pandas",
      "library:mlcroissant",
      "library:polars",
      "region:us"
    ],
    "createdAt": "2025-02-17T11:45:09+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Tabular",
      "Text"
    ]
  },
  {
    "_id": "656d7a05d848a6683a0c5c75",
    "id": "m-a-p/COIG-CQIA",
    "author": "m-a-p",
    "disabled": false,
    "gated": false,
    "lastModified": "2024-04-18T12:10:58+00:00",
    "likes": 633,
    "private": false,
    "sha": "8b55868c6168adf86c30e7ca0f782cca1c514297",
    "description": "\n    \n      \n    \n\n\n\n\t\n\t\t\n\t\tCOIG-CQIA：Quality is All you need for Chinese Instruction Fine-tuning\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n欢迎来到COIG-CQIA，COIG-CQIA全称为Chinese Open Instruction Generalist - Quality is All You Need， 是一个开源的高质量指令微调数据集，旨在为中文NLP社区提供高质量且符合人类交互行为的指令微调数据。COIG-CQIA以中文互联网获取到的问答及文章作为原始数据，经过深度清洗、重构及人工审核构建而成。本项目受LIMA: Less Is More for Alignment等研究启发，使用少量高质量的数据即可让大语言模型学习到人类交互行为，因此在数据构建中我们十分注重数据的来源、质量与多样性，数据集详情请见数据介绍以及我们接下来的论文。\nWelcome to the COIG-CQIA… See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/COIG-CQIA.",
    "downloads": 4261,
    "tags": [
      "task_categories:question-answering",
      "task_categories:text-classification",
      "task_categories:text-generation",
      "task_categories:text2text-generation",
      "language:zh",
      "size_categories:10K<n<100K",
      "format:json",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2403.18058",
      "arxiv:2304.07987",
      "arxiv:2307.09705",
      "region:us"
    ],
    "createdAt": "2023-12-04T07:04:37+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "666ae33f611afe17cd982829",
    "id": "BAAI/Infinity-Instruct",
    "author": "BAAI",
    "disabled": false,
    "gated": "auto",
    "lastModified": "2025-05-15T09:10:26+00:00",
    "likes": 629,
    "private": false,
    "sha": "7271ff8f15bfc99fce0a825f5f1b1193ffa80a93",
    "description": "\n\t\n\t\t\n\t\tInfinity Instruct\n\t\n\n\n\n\n\nBeijing Academy of Artificial Intelligence (BAAI)\n[Paper][Code][🤗] (would be released soon)\n\n\nThe quality and scale of instruction data are crucial for model performance. Recently, open-source models have increasingly relied on fine-tuning datasets comprising millions of instances, necessitating both high quality and large scale. However, the open-source community has long been constrained by the high costs associated with building such extensive and… See the full description on the dataset page: https://huggingface.co/datasets/BAAI/Infinity-Instruct.",
    "downloads": 3134,
    "tags": [
      "task_categories:text-generation",
      "language:en",
      "language:zh",
      "license:cc-by-sa-4.0",
      "size_categories:10M<n<100M",
      "format:parquet",
      "modality:tabular",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2402.00530",
      "arxiv:2405.19327",
      "arxiv:2409.07045",
      "arxiv:2408.07089",
      "region:us"
    ],
    "createdAt": "2024-06-13T12:17:03+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Tabular",
      "Text"
    ]
  },
  {
    "_id": "65d2675495e8d86e2fe4124d",
    "id": "HuggingFaceTB/cosmopedia",
    "author": "HuggingFaceTB",
    "disabled": false,
    "gated": false,
    "lastModified": "2024-08-12T22:05:49+00:00",
    "likes": 616,
    "private": false,
    "sha": "0ae6ec63f91742bd2d1eaef4f02232c55d719385",
    "description": "\n\t\n\t\t\n\t\tCosmopedia v0.1\n\t\n\n\n    \n    Image generated by DALL-E, the prompt was generated by Mixtral-8x7B-Instruct-v0.1\n\n\nNote: Cosmopedia v0.2 is available at smollm-corpus\nUser: What do you think \"Cosmopedia\" could mean? Hint: in our case it's not related to cosmology.\n\nMixtral-8x7B-Instruct-v0.1: A possible meaning for \"Cosmopedia\" could be an encyclopedia or collection of information about\ndifferent cultures, societies, and topics from around the world, emphasizing diversity and global… See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceTB/cosmopedia.",
    "downloads": 6867,
    "tags": [
      "language:en",
      "license:apache-2.0",
      "size_categories:10M<n<100M",
      "format:parquet",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2309.05463",
      "arxiv:2306.11644",
      "region:us",
      "synthetic"
    ],
    "createdAt": "2024-02-18T20:23:48+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  },
  {
    "_id": "621ffdd236468d709f18200b",
    "id": "legacy-datasets/wikipedia",
    "author": "legacy-datasets",
    "disabled": false,
    "gated": false,
    "lastModified": "2024-03-11T18:16:32+00:00",
    "likes": 593,
    "private": false,
    "sha": "97a0b052c326b45fb68593a14972d9eed884cd17",
    "citation": "@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}",
    "description": "Wikipedia dataset containing cleaned articles of all languages.\nThe datasets are built from the Wikipedia dump\n(https://dumps.wikimedia.org/) with one split per language. Each example\ncontains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).",
    "downloads": 25830,
    "tags": [
      "task_categories:text-generation",
      "task_categories:fill-mask",
      "task_ids:language-modeling",
      "task_ids:masked-language-modeling",
      "annotations_creators:no-annotation",
      "language_creators:crowdsourced",
      "multilinguality:multilingual",
      "source_datasets:original",
      "language:aa",
      "language:ab",
      "language:ace",
      "language:af",
      "language:ak",
      "language:als",
      "language:am",
      "language:an",
      "language:ang",
      "language:ar",
      "language:arc",
      "language:arz",
      "language:as",
      "language:ast",
      "language:atj",
      "language:av",
      "language:ay",
      "language:az",
      "language:azb",
      "language:ba",
      "language:bar",
      "language:bcl",
      "language:be",
      "language:bg",
      "language:bh",
      "language:bi",
      "language:bjn",
      "language:bm",
      "language:bn",
      "language:bo",
      "language:bpy",
      "language:br",
      "language:bs",
      "language:bug",
      "language:bxr",
      "language:ca",
      "language:cbk",
      "language:cdo",
      "language:ce",
      "language:ceb",
      "language:ch",
      "language:cho",
      "language:chr",
      "language:chy",
      "language:ckb",
      "language:co",
      "language:cr",
      "language:crh",
      "language:cs",
      "language:csb",
      "language:cu",
      "language:cv",
      "language:cy",
      "language:da",
      "language:de",
      "language:din",
      "language:diq",
      "language:dsb",
      "language:dty",
      "language:dv",
      "language:dz",
      "language:ee",
      "language:el",
      "language:eml",
      "language:en",
      "language:eo",
      "language:es",
      "language:et",
      "language:eu",
      "language:ext",
      "language:fa",
      "language:ff",
      "language:fi",
      "language:fj",
      "language:fo",
      "language:fr",
      "language:frp",
      "language:frr",
      "language:fur",
      "language:fy",
      "language:ga",
      "language:gag",
      "language:gan",
      "language:gd",
      "language:gl",
      "language:glk",
      "language:gn",
      "language:gom",
      "language:gor",
      "language:got",
      "language:gu",
      "language:gv",
      "language:ha",
      "language:hak",
      "language:haw",
      "language:he",
      "language:hi",
      "language:hif",
      "language:ho",
      "language:hr",
      "language:hsb",
      "language:ht",
      "language:hu",
      "language:hy",
      "language:ia",
      "language:id",
      "language:ie",
      "language:ig",
      "language:ii",
      "language:ik",
      "language:ilo",
      "language:inh",
      "language:io",
      "language:is",
      "language:it",
      "language:iu",
      "language:ja",
      "language:jam",
      "language:jbo",
      "language:jv",
      "language:ka",
      "language:kaa",
      "language:kab",
      "language:kbd",
      "language:kbp",
      "language:kg",
      "language:ki",
      "language:kj",
      "language:kk",
      "language:kl",
      "language:km",
      "language:kn",
      "language:ko",
      "language:koi",
      "language:krc",
      "language:ks",
      "language:ksh",
      "language:ku",
      "language:kv",
      "language:kw",
      "language:ky",
      "language:la",
      "language:lad",
      "language:lb",
      "language:lbe",
      "language:lez",
      "language:lfn",
      "language:lg",
      "language:li",
      "language:lij",
      "language:lmo",
      "language:ln",
      "language:lo",
      "language:lrc",
      "language:lt",
      "language:ltg",
      "language:lv",
      "language:lzh",
      "language:mai",
      "language:mdf",
      "language:mg",
      "language:mh",
      "language:mhr",
      "language:mi",
      "language:min",
      "language:mk",
      "language:ml",
      "language:mn",
      "language:mr",
      "language:mrj",
      "language:ms",
      "language:mt",
      "language:mus",
      "language:mwl",
      "language:my",
      "language:myv",
      "language:mzn",
      "language:na",
      "language:nah",
      "language:nan",
      "language:nap",
      "language:nds",
      "language:ne",
      "language:new",
      "language:ng",
      "language:nl",
      "language:nn",
      "language:no",
      "language:nov",
      "language:nrf",
      "language:nso",
      "language:nv",
      "language:ny",
      "language:oc",
      "language:olo",
      "language:om",
      "language:or",
      "language:os",
      "language:pa",
      "language:pag",
      "language:pam",
      "language:pap",
      "language:pcd",
      "language:pdc",
      "language:pfl",
      "language:pi",
      "language:pih",
      "language:pl",
      "language:pms",
      "language:pnb",
      "language:pnt",
      "language:ps",
      "language:pt",
      "language:qu",
      "language:rm",
      "language:rmy",
      "language:rn",
      "language:ro",
      "language:ru",
      "language:rue",
      "language:rup",
      "language:rw",
      "language:sa",
      "language:sah",
      "language:sat",
      "language:sc",
      "language:scn",
      "language:sco",
      "language:sd",
      "language:se",
      "language:sg",
      "language:sgs",
      "language:sh",
      "language:si",
      "language:sk",
      "language:sl",
      "language:sm",
      "language:sn",
      "language:so",
      "language:sq",
      "language:sr",
      "language:srn",
      "language:ss",
      "language:st",
      "language:stq",
      "language:su",
      "language:sv",
      "language:sw",
      "language:szl",
      "language:ta",
      "language:tcy",
      "language:tdt",
      "language:te",
      "language:tg",
      "language:th",
      "language:ti",
      "language:tk",
      "language:tl",
      "language:tn",
      "language:to",
      "language:tpi",
      "language:tr",
      "language:ts",
      "language:tt",
      "language:tum",
      "language:tw",
      "language:ty",
      "language:tyv",
      "language:udm",
      "language:ug",
      "language:uk",
      "language:ur",
      "language:uz",
      "language:ve",
      "language:vec",
      "language:vep",
      "language:vi",
      "language:vls",
      "language:vo",
      "language:vro",
      "language:wa",
      "language:war",
      "language:wo",
      "language:wuu",
      "language:xal",
      "language:xh",
      "language:xmf",
      "language:yi",
      "language:yo",
      "language:yue",
      "language:za",
      "language:zea",
      "language:zh",
      "language:zu",
      "license:cc-by-sa-3.0",
      "license:gfdl",
      "size_categories:n<1K",
      "region:us"
    ],
    "createdAt": "2022-03-02T23:29:22+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": []
  },
  {
    "_id": "667ee649a7d8b1deba8d4f4c",
    "id": "proj-persona/PersonaHub",
    "author": "proj-persona",
    "disabled": false,
    "gated": false,
    "lastModified": "2025-03-04T22:01:42+00:00",
    "likes": 587,
    "private": false,
    "sha": "600b0189027c804fc9373b4de4875c171656a4df",
    "description": "\n\t\n\t\t\n\t\tScaling Synthetic Data Creation with 1,000,000,000 Personas\n\t\n\nThis repo releases data introduced in our paper Scaling Synthetic Data Creation with 1,000,000,000 Personas:\nWe propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce PERSONA HUB – a collection of 1 billion diverse personas automatically curated from web data.… See the full description on the dataset page: https://huggingface.co/datasets/proj-persona/PersonaHub.",
    "downloads": 6136,
    "tags": [
      "task_categories:text-generation",
      "task_categories:text-classification",
      "task_categories:token-classification",
      "task_categories:fill-mask",
      "task_categories:table-question-answering",
      "task_categories:text2text-generation",
      "language:en",
      "language:zh",
      "license:cc-by-nc-sa-4.0",
      "size_categories:100K<n<1M",
      "format:json",
      "modality:text",
      "library:datasets",
      "library:dask",
      "library:mlcroissant",
      "library:polars",
      "arxiv:2406.20094",
      "region:us",
      "synthetic",
      "text",
      "math",
      "reasoning",
      "instruction",
      "tool"
    ],
    "createdAt": "2024-06-28T16:35:21+00:00",
    "key": "",
    "viewer_sample_rows": [],
    "modalities": [
      "Text"
    ]
  }
]